SkinBench v2 – Multilayer Skin Disease Classification

This repository contains SkinBench v2, an improved multilayer classification framework for automated dermatology analysis. It builds upon the original SkinBench project and adds new models (EfficientNet and VGG16/19), a subclass prediction head, updated evaluation tools and a reproducible LaTeX report. The entire pipeline is organised so that GitHub can automatically render all results (images and tables) when you browse the v2 branch.

Repository structure
SkinBench/
├── data_raw/              # original dataset (train/val/test per class)
├── models/                # PyTorch model definitions
├── eval_tools/            # evaluation scripts (ROC/PR curves, calibration, etc.)
├── runs/                  # training outputs
│   ├── ALL9/              # results for the 9‑class ALL9 phase
│   │   └── figures/       # confusion matrices, ROC/PR and calibration plots
│   ├── L1/                # results for binary (Normal vs Abnormal) phase
│   ├── L2/                # results for 8‑class abnormal phase
│   ├── SUBCLASS/          # results for subclass classifier
│   └── tables/            # CSV summaries (leaderboard, confusion tables, predictions)
├── figures/               # copy of selected figures used in the LaTeX report
├── table/                 # copy of selected tables used in the LaTeX report
├── main.tex               # IEEE‑style report source
└── reference.bib          # bibliography

Runs directory

The runs/ folder is automatically generated by training and evaluation scripts. It contains the best model checkpoints along with figures and tables summarising the results. Within each phase (ALL9, L1, L2), a figures/ folder holds PNG images for:

Confusion matrices – e.g. confusion_ALL9_cnn.png, confusion_ALL9_efficientnet.png.

ROC curves – one‑vs‑rest receiver‑operating characteristic curves such as roc_ovr_ALL9_swin_densenet.png.

PR curves – precision–recall curves, e.g. pr_ovr_ALL9_vit_resnet.png.

Calibration (reliability) diagrams – e.g. reliability_ALL9_mobilenetv3.png.

For the binary L1 phase you will also find roc_binary_L1_resnet50.png, pr_binary_L1_resnet50.png and reliability_L1_resnet50.png.

The runs/tables/ directory collects comma‑separated files summarising the experiments:

comparison_all_models.csv – a leaderboard of all models and phases with accuracy, macro‑F1 and AUC scores.

confusion_ALL9_*.csv – raw confusion matrix counts for each ALL9 model (CNN, EfficientNet, MobileNetV3, Swin‑DenseNet, VGG16/19, ViT‑ResNet).

confusion_L1_resnet50.csv and confusion_L2_*.csv – confusion matrices for L1 and L2 phases.

pred_ALL9_*.csv – per‑image predictions (probabilities and true labels) for each ALL9 model.

pred_L1_resnet50.csv and pred_L2_*.csv – per‑image predictions for L1 and L2 phases.

These CSV files can be opened directly in GitHub (or downloaded) to inspect the raw numbers behind each figure.

Viewing results on GitHub

When you push this repository to GitHub and switch to the v2 branch, the plots under runs/ALL9/figures and runs/tables will be displayed automatically in the web UI. To ensure GitHub can find them, use relative links when referencing images in markdown or the report. For example, to embed the ALL9 CNN confusion matrix in your README or wiki page, use:

![CNN confusion matrix](runs/ALL9/figures/confusion_ALL9_cnn.png)


Similarly, the leaderboard CSV can be linked as:

[Model comparison](runs/tables/comparison_all_models.csv)


GitHub will render these resources without any extra configuration. All figures used in the LaTeX report have been copied into the top‑level figures/ and table/ folders to keep the manuscript self‑contained; the originals remain in runs/ for interactive browsing.

Training and evaluation

To reproduce the results:

Install dependencies – create a Python environment and install packages listed in requirements.txt (PyTorch, torchvision, pandas, scikit‑learn, etc.).

Run training using train_multilayer.py. For example, to train L1 ResNet50:

python train_multilayer.py --data_dir data_raw --phase L1 --model resnet50


Similarly, replace L1 with L2 or ALL9 and choose from resnet50, densenet121, mobilenetv3, efficientnet, cnn, vit_resnet, swin_densenet, vgg16 or vgg19.

Evaluate models with the scripts in eval_tools. The unified run_all_evals.py will run confusion matrices, ROC/PR, calibration, and write the results to runs/:

python -m eval_tools.run_all_evals --data_dir data_raw --phases ALL9 L1 L2 --models resnet50 densenet121 mobilenetv3 cnn vit_resnet swin_densenet efficientnet vgg16 vgg19


Generate the report – compile main.tex with LaTeX (e.g. pdflatex or Overleaf). The report includes the figures from figures/ and tables from table/.

Launch the demo – use streamlit run app.py to test the live prediction interface. The app loads your best models from runs/ and supports subclass prediction under Eczema, Fungal Infections and Pox.

Contributing

Fork this repository, create a branch (e.g. feature/new-model), commit your changes and submit a pull request. Please update the documentation and add new evaluation plots and tables under runs/ so that others can reproduce your results.

Acknowledgements

SkinBench v2 was developed at Daffodil International University as part of a bachelor thesis. We thank Bokkobidi Hospital for providing clinical guidance and the open‑source community for the models used in this project.